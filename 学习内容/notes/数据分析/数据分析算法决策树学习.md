# 决策树

名字很正式，但就是一个树，在这个树上有很多树枝，并且这颗树可以来帮忙做决定。

从下面几个步骤来了解什么是决策树

- 工作原理构造
- 工作原理剪枝

## 构造

想要一棵树，就得先把树构造出来，那世上那么多种类的树，怎么才能构造一颗决策树呢？这就是在构造树的过程中需要考虑的。

1. 树都有头部，对应的决策树就会有**根节点**，其实就是树的最顶端，开始的那个节点。
2. 树分叉，存在多个节点，这些节点就构成了决策树的**内部节点**。
3. 叶节点，就是树的底部的节点，对应决策树就是最后的结果。

从上面看主要分为了三类节点，根节点，内部节点，叶节点。但是三者之间就构成了多个关系。
根节点下面可以有多个内部节点（第一代），该部分内部节点可以称为子节点，而子节点的子节点可以称为子子节点（第二代），但这些都属于内部节点。最后一层的节点被称为**叶节点，它没有子节点**。

所以构建决策树考虑的是哪个内容是作为**根节点(总体情况)**，哪些是作为**子节点(中间的各种情况)**，哪些是作为**叶节点(决策结果)**。

内部节点可以认为是根节点中的某个情况。

## 剪枝

在决策树中，有一个情况叫做剪枝. 自信是好，但是自信过头就是自傲，而不自信就自卑。自傲与自卑是两个极端的情况，同样的在决策树中也有这样的情况。

### 为什么要剪枝

决策树构建的太好，就会出现分类的结果太好的情况（会将一些不正确的数据进行了错误的分类）这种情景在算法中称为过拟合。构建的太差，更会导致分类的结果错误。

那什么情况会造成过拟合呢？

1. 给出的训练数据小
2. 在构造决策树的时候，选择的属性过多。比如说**训练集**一共10个属性我们构建的时候就把10个属性全部算进去，那么程序一定能把训练集的数据都分类成功，而训练集只是真实数据的一部分，容易导致真实数据分类出现错误。
3. 上面出现错误导致模型的可扩展性降低，通过一个例子去思考更多的可能性，可以认为是举一反三的能力。降低该能力训练出来的模型就会在真实数据中出现错误的判断。

### 解决问题

解决上面的过拟合或者模型的泛化能力差在决策树中使用了**剪枝的方法**。

剪枝分为 **预剪枝和后剪枝**。

#### 预剪枝

预剪枝就是在构建决策树时，对子节点的划分就需要注意，当子节点对于结果不能带来准确性的提高就不能将其划分为子节点。，这就是预剪枝，在构建节点时需要考虑。

#### 后剪枝

决策树构建完成后，从上往下对每个节点进行评估（判断这个节点对结果的准确定影响成功，区别不大，就可以将其减去）。

后剪枝操作减去节点后，下面有子节点会将其位置代替。再次评估节点

从那个上面看，不管使用预剪枝还是后剪枝都是为了提高最后的决策树做的决策准确率，以结果为标准。

## 总结

明白决策树的构建原理，后面用次做数据分析的时候才能更好的根据众多属性来分析文章内容。




