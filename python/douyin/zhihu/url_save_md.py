
# è¿è¡Œç¨‹åº python3 url_save_md.py
#!/usr/bin/python3

from copy import deepcopy
import os
import requests
from pyquery import PyQuery as pq
from bs4 import BeautifulSoup

#from yuyin import output


output_path =os.path.join(os.getenv("ROOT_PATH","/workspaces/notes/python/douyin/output"), os.getenv(
    'OUTPUT_PATH', "zhihu/booking/"))

booking_size = int(os.getenv("BOOKING_SIZE",1800))
agent = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Mobile Safari/537.36'

headers = {
    'User-Agent': agent
}

def getText(url,video_title):

    html = requests.get(url, headers=headers).text

    reqs = requests.get(url, headers=headers)

    doc = pq(html)

    text = doc("p").text()

    text = "\n".join(text.split())

    soup = BeautifulSoup(reqs.text, 'html.parser')

    print("Title of the website is : ")

    urlTiele = ''

    for title in soup.find_all('title'):

        urlTiele = title.get_text()

        print(title.get_text())
    # è·å–é—®ç­”
    #text = soup.find(attrs={"itemprop": "text"})
    # è·å–ä¸“æ 
    text = soup.find(id="manuscript")

    texts = []
    p_text = text.find_all('p')
    for p in p_text:
        print(p.get_text())
        texts.append(p.get_text())
    content = text.get_text()
    print("è·å–åˆ°æ•°æ®",content)

    textName2 = urlTiele

    name2 = output_path + textName2 + '.md'
    origin_content = deepcopy(content)
#    content = content.replace('\n','')
    # æ›¿æ¢æ— ç”¨çš„æ•°å­—
    t = content.replace('ã€Œ', '').replace('ã€', '').replace('ã€‚', 'ï¼Œ').replace(
        'ï¼Ÿ', 'ï¼Œ').replace('â€¦â€¦', 'ï¼Œ').replace('â€', '').replace('â€œ', '').replace('ã€','').replace('ã€','').replace(
        ']','').replace('[','').replace(':','ï¼Œ').replace("ï¼š",'ï¼Œ').replace('?','ï¼Œ').replace("â€”â€”",'ï¼Œ').replace(
        "ï¼Œï¼Œ","ï¼Œ").replace("ï¼",'ï¼Œ').replace('.','').replace('ï¼Œï¼Œ','ï¼Œ').replace('æœ€å¥½','å˜´å¥½').replace(
        'ç ','çœ‹').replace('ç ¸','æ‚').replace('1\n','').replace('2\n','').replace('3\n','').replace('4\n','').replace(
        '5\n','').replace('6\n','').replace('7\n','').replace('8\n','').replace('9\n','').replace('0','').replace('ã€‘','').replace(
            "ã€",'').replace(' ','').replace('ã€€','').strip()
        
    # for index in range(1,20):
    #    num = str(index) + "."
    #    t =t.replace(num,"\n" +num + "\n") 
    # t =t.replace('0\n',"\n") 
       
    print("è¾“å‡ºæ–‡ä»¶å†…å®¹ï¼š\n", t)  # è¾“å‡ºæ–‡ä»¶å†…å®¹

    #å¤„ç†æ•æ„Ÿè¯
    t = t.replace('æ‹å–','æŸºå–').replace('æ­»','4').replace('ä¸‹èº«','ä¸‹æ¸—').replace('æˆ‘é ','').replace(
        'è­¦å¯Ÿ','ç»æŸ¥').replace("æ‰‹æª",'é¦–æŠ¢').replace("ç­¹ç ",'æŠ½ç ').replace("ä½ ä»–å¦ˆ","ä½ ").replace("ä»–å¦ˆ",'').replace(
            'æ‚ç¢','ç ¸ç¢').replace('ç¬¬ä¸€','å¸ä¸€').replace("æè‡´",'æœºæ™º').replace("å®‰å…¨",'æ¡ˆå…¨').replace("æœ€",'å˜´').replace('ã',
            "æ€‚").replace("å…¨é¢",'å…¨æ£‰').replace('å”¯ä¸€','æƒŸä¸€').replace("ä¼˜ç§€","å³è¢–").replace("å…¨å›½","åŠè¿‡").replace("é¡¶çº§","å®šçº§").replace(
                "çƒ§","ğŸ”¥")
        

    # æ›¿æ¢ä½œè€…æ˜µç§°
    t = t.replace("èµµå­å›",'èµµé›…æ€').replace("æ²ˆä¿®ç™½",'æ²ˆå…ƒç™½')
    # å¢åŠ å‰ç¼€
    t = ""  + t
    total_size = len(t)
    # æŒ‰ç…§1600æ¥åˆ†å‰²
    split_size = 1450
    split_result = ''
    start_index = 128
    end_index =1550
    # ä¿®æ”¹å‰ç¼€
    page_prefix = t[0:start_index]
    
    index_prefix = 1
    while True:
        current_content =""
        if end_index >= total_size:
            index_prefix+=1
            #current_content =page_prefix + t[start_index:total_size]
            split_result += page_prefix +t[start_index:total_size]
            print("è¾“å‡ºæ•°æ®",start_index,end_index,total_size)
            break
        elif t[end_index] == 'ï¼Œ':
            index_prefix+=1
            end_index = end_index+1
            print("è¾“å‡ºæ•°æ®",start_index,end_index,total_size)
            current_content = page_prefix + t[start_index:end_index]
            #output(current_content,f"{video_title}({index_prefix})")
            split_result += page_prefix+ t[start_index:end_index] +"\n"
            end_index_bak = end_index
            end_index = end_index +split_size
            start_index = end_index_bak
        else:
            end_index+=1

    t = url + "\n" + split_result
    with open(name2, "w") as f2:
        f2.write(t)
    return origin_content
    # move_folder_name2 = "mdæ–‡ä»¶/" + name2

    # shutil.move(name2,move_folder_name2)

def run_zhuanlan():
    url = 'https://www.zhihu.com/market/paid_column/1543288790588035072/section/1548372885319872512'
    content = getText(url,"")
    # import image2
    # image2.show_image(content)

if __name__ == "__main__":
    run_zhuanlan()
    pass




